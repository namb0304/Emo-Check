{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emo-Check: ResNet152 è»¢ç§»å­¦ç¿’\n",
    "\n",
    "ç”»åƒã®ã€Œã‚¨ãƒ¢ã•ã€ã‚’åˆ¤å®šã™ã‚‹ãŸã‚ã®äºŒå€¤åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ResNet152ãƒ™ãƒ¼ã‚¹ã§å­¦ç¿’ã—ã¾ã™ã€‚\n",
    "\n",
    "- **class_0**: Not Emoï¼ˆã‚¨ãƒ¢ããªã„ç”»åƒï¼‰\n",
    "- **class_1**: Emoï¼ˆã‚¨ãƒ¢ã„ç”»åƒï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install torch torchvision pillow matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# GPUè¨­å®š\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 2\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹\n",
    "DATA_DIR = './dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã¨å‰å‡¦ç†\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿\n",
    "full_dataset = datasets.ImageFolder(DATA_DIR, transform=train_transforms)\n",
    "\n",
    "# å­¦ç¿’/æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰² (80/20)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«ã¯æ‹¡å¼µãªã—ã®å¤‰æ›ã‚’é©ç”¨\n",
    "val_dataset.dataset.transform = val_transforms\n",
    "\n",
    "print(f'Total images: {len(full_dataset)}')\n",
    "print(f'Training images: {train_size}')\n",
    "print(f'Validation images: {val_size}')\n",
    "print(f'Classes: {full_dataset.classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderä½œæˆ\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet152ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ï¼ˆè»¢ç§»å­¦ç¿’ï¼‰\n",
    "model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# æœ€çµ‚å±¤ã‚’äºŒå€¤åˆ†é¡ç”¨ã«ç½®ãæ›ãˆ\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(num_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Linear(512, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(f'Model loaded and moved to {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå¤±é–¢æ•°ã¨æœ€é©åŒ–\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "# å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’é–¢æ•°\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100.*correct/total:.2f}%'})\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¤œè¨¼é–¢æ•°\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Validation'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’ãƒ«ãƒ¼ãƒ—\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'\\n=== Epoch {epoch+1}/{NUM_EPOCHS} ===')\n",
    "    \n",
    "    # å­¦ç¿’\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # æ¤œè¨¼\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # å­¦ç¿’ç‡æ›´æ–°\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    print(f'Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
    "    \n",
    "    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "        }, 'resnet152_best.pth')\n",
    "        print(f'âœ“ Best model saved! (Val Acc: {val_acc:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’æ›²ç·šã®ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(train_losses, label='Train Loss', color='#ff6b6b', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Val Loss', color='#4ecdc4', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(train_accs, label='Train Acc', color='#ff6b6b', linewidth=2)\n",
    "axes[1].plot(val_accs, label='Val Acc', color='#4ecdc4', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('resnet152_training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nğŸ‰ Training Complete!')\n",
    "print(f'Best Validation Accuracy: {best_val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨è«–ç”¨ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆstate_dictã®ã¿ï¼‰\n",
    "# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "checkpoint = torch.load('resnet152_best.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# æ¨è«–ç”¨ã«ä¿å­˜\n",
    "torch.save(model.state_dict(), 'resnet152.pth')\n",
    "print('âœ“ Model saved as resnet152.pth')\n",
    "print(f'  - Best Epoch: {checkpoint[\"epoch\"] + 1}')\n",
    "print(f'  - Val Accuracy: {checkpoint[\"val_acc\"]:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨è«–ãƒ†ã‚¹ãƒˆ\n",
    "from PIL import Image\n",
    "\n",
    "def predict_emo_score(image_path, model, device):\n",
    "    \"\"\"å˜ä¸€ç”»åƒã®ã‚¨ãƒ¢åº¦ã‚’äºˆæ¸¬\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        emo_score = probabilities[0][1].item()  # class_1 (Emo) ã®ç¢ºç‡\n",
    "    \n",
    "    return emo_score * 100  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã§è¿”ã™\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆç”¨: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ç”»åƒã§ãƒ†ã‚¹ãƒˆï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n",
    "# score = predict_emo_score('path/to/test/image.jpg', model, device)\n",
    "# print(f'Emo Score: {score:.1f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
