{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emo-Check: Vision Transformer (ViT-B/16) è»¢ç§»å­¦ç¿’\n",
    "\n",
    "ç”»åƒã®ã€Œã‚¨ãƒ¢ã•ã€ã‚’åˆ¤å®šã™ã‚‹ãŸã‚ã®äºŒå€¤åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ViT-B/16ãƒ™ãƒ¼ã‚¹ã§å­¦ç¿’ã—ã¾ã™ã€‚\n",
    "\n",
    "- **class_0**: Not Emoï¼ˆã‚¨ãƒ¢ããªã„ç”»åƒï¼‰\n",
    "- **class_1**: Emoï¼ˆã‚¨ãƒ¢ã„ç”»åƒï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "!pip install torch torchvision pillow matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# GPUè¨­å®š\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "BATCH_SIZE = 16  # ViTã¯ãƒ¡ãƒ¢ãƒªã‚’å¤šãä½¿ç”¨ã™ã‚‹ãŸã‚å°ã•ã‚ã«\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.0001  # ViTã¯å°ã•ã„å­¦ç¿’ç‡ãŒåŠ¹æœçš„\n",
    "NUM_CLASSES = 2\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹\n",
    "DATA_DIR = './dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã¨å‰å‡¦ç†ï¼ˆViTç”¨ï¼‰\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿\n",
    "full_dataset = datasets.ImageFolder(DATA_DIR, transform=train_transforms)\n",
    "\n",
    "# å­¦ç¿’/æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰² (80/20)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«ã¯æ‹¡å¼µãªã—ã®å¤‰æ›ã‚’é©ç”¨\n",
    "val_dataset.dataset.transform = val_transforms\n",
    "\n",
    "print(f'Total images: {len(full_dataset)}')\n",
    "print(f'Training images: {train_size}')\n",
    "print(f'Validation images: {val_size}')\n",
    "print(f'Classes: {full_dataset.classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderä½œæˆ\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision Transformer (ViT-B/16) ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ï¼ˆè»¢ç§»å­¦ç¿’ï¼‰\n",
    "model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# æœ€çµ‚å±¤ï¼ˆheadsï¼‰ã‚’äºŒå€¤åˆ†é¡ç”¨ã«ç½®ãæ›ãˆ\n",
    "num_features = model.heads.head.in_features\n",
    "model.heads.head = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(num_features, 256),\n",
    "    nn.GELU(),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Linear(256, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(f'ViT-B/16 model loaded and moved to {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå¤±é–¢æ•°ã¨æœ€é©åŒ–\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ViTç”¨ã®æœ€é©åŒ–è¨­å®šï¼ˆç•°ãªã‚‹å­¦ç¿’ç‡ã‚’å±¤ã”ã¨ã«é©ç”¨ï¼‰\n",
    "# ãƒ˜ãƒƒãƒ‰éƒ¨åˆ†ã¯é«˜ã„å­¦ç¿’ç‡ã€äº‹å‰å­¦ç¿’éƒ¨åˆ†ã¯ä½ã„å­¦ç¿’ç‡\n",
    "param_groups = [\n",
    "    {'params': [p for n, p in model.named_parameters() if 'heads' not in n], 'lr': LEARNING_RATE * 0.1},\n",
    "    {'params': model.heads.parameters(), 'lr': LEARNING_RATE}\n",
    "]\n",
    "optimizer = optim.AdamW(param_groups, weight_decay=0.05)\n",
    "\n",
    "# Warmupä»˜ãã‚³ã‚µã‚¤ãƒ³ã‚¢ãƒ‹ãƒ¼ãƒªãƒ³ã‚°\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’é–¢æ•°\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆViTã®å®‰å®šæ€§å‘ä¸Šï¼‰\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100.*correct/total:.2f}%'})\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¤œè¨¼é–¢æ•°\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Validation'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’ãƒ«ãƒ¼ãƒ—\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'\\n=== Epoch {epoch+1}/{NUM_EPOCHS} ===')\n",
    "    \n",
    "    # å­¦ç¿’\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # æ¤œè¨¼\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # å­¦ç¿’ç‡æ›´æ–°\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Learning Rate (backbone): {current_lr:.6f}')\n",
    "    \n",
    "    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss,\n",
    "        }, 'vit_b16_best.pth')\n",
    "        print(f'âœ“ Best model saved! (Val Acc: {val_acc:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­¦ç¿’æ›²ç·šã®ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(train_losses, label='Train Loss', color='#a855f7', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Val Loss', color='#06b6d4', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('ViT-B/16: Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(train_accs, label='Train Acc', color='#a855f7', linewidth=2)\n",
    "axes[1].plot(val_accs, label='Val Acc', color='#06b6d4', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('ViT-B/16: Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vit_b16_training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nğŸ‰ Training Complete!')\n",
    "print(f'Best Validation Accuracy: {best_val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨è«–ç”¨ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆstate_dictã®ã¿ï¼‰\n",
    "# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "checkpoint = torch.load('vit_b16_best.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# æ¨è«–ç”¨ã«ä¿å­˜\n",
    "torch.save(model.state_dict(), 'vit_b16.pth')\n",
    "print('âœ“ Model saved as vit_b16.pth')\n",
    "print(f'  - Best Epoch: {checkpoint[\"epoch\"] + 1}')\n",
    "print(f'  - Val Accuracy: {checkpoint[\"val_acc\"]:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨è«–ãƒ†ã‚¹ãƒˆ\n",
    "from PIL import Image\n",
    "\n",
    "def predict_emo_score_vit(image_path, model, device):\n",
    "    \"\"\"å˜ä¸€ç”»åƒã®ã‚¨ãƒ¢åº¦ã‚’äºˆæ¸¬ï¼ˆViTç”¨ï¼‰\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        emo_score = probabilities[0][1].item()  # class_1 (Emo) ã®ç¢ºç‡\n",
    "    \n",
    "    return emo_score * 100  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã§è¿”ã™\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆç”¨: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ç”»åƒã§ãƒ†ã‚¹ãƒˆï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰\n",
    "# score = predict_emo_score_vit('path/to/test/image.jpg', model, device)\n",
    "# print(f'Emo Score: {score:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Mapã®å¯è¦–åŒ–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰\n",
    "def visualize_attention(model, image_path, device):\n",
    "    \"\"\"ViTã®Attentionã‚’å¯è¦–åŒ–\"\"\"\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Attention weightã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ãƒ•ãƒƒã‚¯\n",
    "    attention_weights = []\n",
    "    \n",
    "    def get_attention(module, input, output):\n",
    "        attention_weights.append(output)\n",
    "    \n",
    "    # æœ€å¾Œã®Transformerãƒ–ãƒ­ãƒƒã‚¯ã®self-attentionã«ãƒ•ãƒƒã‚¯ã‚’è¿½åŠ \n",
    "    # æ³¨: ã“ã®å®Ÿè£…ã¯torchvisionã®ViTã®å†…éƒ¨æ§‹é€ ã«ä¾å­˜\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        emo_score = probabilities[0][1].item()\n",
    "    \n",
    "    return emo_score * 100\n",
    "\n",
    "print('Attention visualization function defined (optional use)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
